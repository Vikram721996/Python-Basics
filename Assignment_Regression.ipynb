{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression**"
      ],
      "metadata": {
        "id": "GXfWmHAqv1LP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Questions**"
      ],
      "metadata": {
        "id": "4KBBb1miwH9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "91bzFhe9wKmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable $(Y)$ and a single independent variable $(X)$ using a straight-line equation: $Y = mX + b,$ where m is the slope and $b$ is the intercept. It helps predict values of $Y$ based on $X$ and is commonly used for trend analysis and forecasting."
      ],
      "metadata": {
        "id": "DpGbDnyKW2uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the key assumptions of Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "-GIf4KkdwPGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression relies on several key assumptions for accurate predictions:\n",
        "\n",
        "**1. Linearity -** The relationship between the dependent and independent variable is linear.\n",
        "\n",
        "**2. Independence -** Observations are independent of each other.\n",
        "\n",
        "**3. Homoscedasticity -** The variance of residuals (errors) remains constant across all values of the independent variable.\n",
        "\n",
        "**4. Normality of Residuals -** The residuals (errors) are normally distributed.\n",
        "\n",
        "**5. No Multicollinearity -** Since it's simple linear regression, there's only one independent variable, so multicollinearity isn't a concern."
      ],
      "metadata": {
        "id": "bZqAa8qEXV9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What does the coefficient m represent in the equation $Y=mX+c$?**"
      ],
      "metadata": {
        "id": "YKabksIWwU3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the linear equation $Y = mX + c$, the coefficient **'m'** represents the slope of the line. It indicates how much Y changes for every unit increase in X."
      ],
      "metadata": {
        "id": "8Oc0LY0o4HqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What does the intercept $c$ represent in the equation $Y=mX+c$?**"
      ],
      "metadata": {
        "id": "GHSPjFd2weEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the linear equation $Y = mX + c$, the intercept **'c'** represents the value of $Y$ when $X$ is zero. In other words, it's the point where the line crosses the $Y-axis.$"
      ],
      "metadata": {
        "id": "bodK9_R1435w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. How do we calculate the slope m in Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "5uGYdy41wuFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope $'m'$ in Simple Linear Regression is calculated using the least squares method.\n",
        "This method finds the unique line that minimizes the sum of the squared vertical distances (residuals) between each data point and the line itself.\n",
        "\n",
        "By minimizing these squared errors, the process mathematically determines the most appropriate value for $'m'$ that best fits the linear trend in the data."
      ],
      "metadata": {
        "id": "wg4I7DuA6YSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. What is the purpose of the least squares method in Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "TXeM5CBZw2B2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Finding the Best Fit:** It determines the optimal line that represents the relationship between X and Y with minimal error.\n",
        "\n",
        "**2. Minimizing Errors:** By squaring the differences, it avoids canceling out positive and negative errors, ensuring an accurate representation.\n",
        "\n",
        "**3. Improving Predictions:** A well-fitted regression line provides reliable predictions for unseen data points.\n",
        "\n",
        "**4. Handling Variability:** It helps account for natural fluctuations in data while identifying underlying trends."
      ],
      "metadata": {
        "id": "3ODBMo6Y63J5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.  How is the coefficient of determination ($R²$) interpreted in Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "-mm_49uWxEGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient of determination (R²) is a measure of how well the regression model explains the variability of the dependent variable (Y) using the independent variable (X). It indicates the strength and reliability of the relationship.\n",
        "\n",
        "**Interpreting R²**\n",
        "\n",
        "* **R² = 1 (100%) -** The model perfectly fits the data; all points lie exactly on the regression line.\n",
        "\n",
        "* **R² near 1 (e.g., 0.8 or 80%) -** The model explains a high proportion of the variation in Y based on X.\n",
        "\n",
        "* **R² near 0 (e.g., 0.2 or 20%) -** The model explains very little of the variation in Y; X has a weak influence.\n",
        "\n",
        "* **R² = 0 -** The model does not explain any variability in Y."
      ],
      "metadata": {
        "id": "KUlQOZeR8D-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What is Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "f9tNdOaExHGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression (MLR) is an extension of Simple Linear Regression, where we model the relationship between a dependent variable (Y) and two or more independent variables (X₁, X₂, X₃, ...). Instead of just one predictor influencing Y, multiple factors contribute to the outcome."
      ],
      "metadata": {
        "id": "GZ_UC2Qn8wNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What is the main difference between Simple and Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "MCgoBuirxLp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used:\n",
        "\n",
        "* **Simple Linear Regression** uses only one independent variable to predict the dependent variable.\n",
        "\n",
        "* **Multiple Linear Regression** uses two or more independent variables to predict the dependent variable."
      ],
      "metadata": {
        "id": "Ka5P9Qi89EZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What are the key assumptions of Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "aIcxND2hxQ7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "* **Linearity**: A linear relationship exists between the dependent variable and each independent variable.\n",
        "* **Independence of Errors**: The residuals (errors) are independent of each other.\n",
        "* **Normality of Errors**: The residuals are normally distributed.\n",
        "* **Homoscedasticity**: The variance of the residuals is constant across all levels of the independent variables.\n",
        "* **No Multicollinearity**: The independent variables are not highly correlated with each other."
      ],
      "metadata": {
        "id": "8cn-YwUz9MwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**"
      ],
      "metadata": {
        "id": "tz5hjaHfxX8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables. In a Multiple Linear Regression model, this condition violates a key assumption (homoscedasticity).\n",
        "\n",
        "**It primarily affects the results by:**\n",
        "\n",
        "1. **Biased Standard Errors**: The standard errors of the regression coefficients become incorrect (usually underestimated), leading to unreliable t-statistics and p-values.\n",
        "2. **Invalid Statistical Inferences**: This makes it difficult to assess the true statistical significance of predictors, potentially leading to incorrect conclusions about which variables are important.\n",
        "3. **Inefficient Estimates**: While coefficients remain unbiased, they are no longer the most efficient, meaning they are not the most precise estimates."
      ],
      "metadata": {
        "id": "rLMBKyuH9ZhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?**"
      ],
      "metadata": {
        "id": "gcq0wyYtxlDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Remove highly correlated predictors.\n",
        "\n",
        "* Combine correlated variables into a composite feature.\n",
        "\n",
        "* Use PCA to reduce dimensionality.\n",
        "\n",
        "* Apply regularization (Ridge/Lasso) to stabilize coefficients.\n",
        "\n",
        "* Increase sample size to minimize collinearity effects.\n",
        "\n",
        "* Perform feature engineering (transformations, interactions).\n",
        "\n",
        "* Ignore it if only prediction matters, but inference is compromised."
      ],
      "metadata": {
        "id": "ZIY14a9j-NJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13. What are some common techniques for transforming categorical variables for use in regression models?**"
      ],
      "metadata": {
        "id": "FfHH-TXkxpjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common techniques for transforming categorical variables for use in regression models include:\n",
        "\n",
        "1.  **One-Hot Encoding:** Creates new binary (0 or 1) columns for each category, indicating its presence. It avoids implying any order and is widely used for nominal categories.\n",
        "2.  **Label Encoding (or Ordinal Encoding):** Assigns a unique integer to each category. This is suitable only for *ordinal* categorical variables where a natural order exists (e.g., 'Low'=1, 'Medium'=2, 'High'=3). Using it for nominal data can mislead the model by implying an artificial order.\n",
        "3.  **Target Encoding (or Mean Encoding):** Replaces each category with the mean of the target variable for that category. It can be powerful but is prone to overfitting and requires careful validation.\n",
        "4.  **Binary Encoding:** Converts categories to integers, then represents those integers in binary code, creating new columns for each bit. It's a compromise between one-hot and label encoding for high-cardinality nominal variables.\n",
        "5.  **Frequency Encoding:** Replaces each category with its frequency or count in the dataset. It's simple and can reduce dimensionality."
      ],
      "metadata": {
        "id": "pWeN--CZ-s2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14. What is the role of interaction terms in Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "DQG_RL9sx5H5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction terms in Multiple Linear Regression capture situations where the effect of one independent variable on the dependent variable depends on the value of another independent variable.\n",
        "\n",
        "Their role is to:\n",
        "\n",
        "* **Model non-additive effects:** They allow the model to represent synergistic or antagonistic relationships where the combined effect of two variables is different from the sum of their individual effects.\n",
        "* **Allow for changing slopes:** An interaction term means that the slope (the strength and direction of the relationship) of one predictor changes as the value of the interacting predictor changes.\n",
        "* **Provide a more nuanced understanding:** They help create a more realistic and sophisticated model by revealing conditional relationships between variables that simple additive models cannot capture.\n",
        "* **Improve model fit:** When present, including relevant interaction terms can significantly enhance the model's explanatory power and predictive accuracy."
      ],
      "metadata": {
        "id": "4lB0cfYz_Prq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "qvFI0swVyFYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The interpretation of the intercept can differ due to the number of independent variables:\n",
        "\n",
        "* **Simple Linear Regression:** The intercept is the predicted value of the dependent variable (Y) when the *single* independent variable (X) is zero. Its interpretation is often straightforward if X=0 is a meaningful value.\n",
        "* **Multiple Linear Regression:** The intercept is the predicted value of Y when *all* independent variables ($X_1, X_2, \\ldots, X_n$) are simultaneously zero. This interpretation can be complex or meaningless if it's unrealistic for all predictors to be zero at once (e.g., house price with 0 square footage and 0 bedrooms). It often represents an extrapolation."
      ],
      "metadata": {
        "id": "-TTlJHE9BL9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?**"
      ],
      "metadata": {
        "id": "A_zMBWiNyIfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The significance of the slope in regression analysis lies in its ability to quantify the estimated relationship between the independent variable(s) and the dependent variable.\n",
        "\n",
        "* It tells us the direction (positive or negative) and magnitude (how much Y changes for a one-unit change in X, holding other variables constant in MLR) of the relationship.\n",
        "* A statistically significant slope (low p-value) indicates that this relationship is unlikely due to random chance, suggesting the independent variable is a meaningful predictor.\n",
        "\n",
        "**How it affects predictions:**\n",
        "The slope directly dictates how the model generates predictions. It determines the steepness and direction of the regression line (or hyperplane), thereby defining the specific predicted value of Y for any given value(s) of X. A larger absolute slope means Y is predicted to change more dramatically for a given change in X."
      ],
      "metadata": {
        "id": "pTXdJgAwBOKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17. How does the intercept in a regression model provide context for the relationship between variables?**"
      ],
      "metadata": {
        "id": "kSri-UpvyO-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intercept in a regression model represents the expected value of the dependent variable when all predictors are zero. It acts as a baseline, adjusts the model's behavior, and helps in comparisons across different models.\n",
        "\n",
        "In some cases, it has a realistic interpretation, while in others, a zero value may be meaningless. Understanding the intercept ensures meaningful insights, especially in models like bike depreciation or car pricing trends!"
      ],
      "metadata": {
        "id": "BQPzIBQ6BsrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18. What are the limitations of using R² as a sole measure of model performance?**"
      ],
      "metadata": {
        "id": "WZssrkMoyVPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using $R^2$ as the sole measure of model performance has several limitations:\n",
        "\n",
        "1.  **Inflates with More Predictors:** $R^2$ *always* increases or stays the same when more independent variables are added, even if they are irrelevant, which can lead to overfitting.\n",
        "2.  **Doesn't Imply Causation:** A high $R^2$ shows correlation, not necessarily a cause-and-effect relationship.\n",
        "3.  **No Information on Prediction Error:** It indicates variance explained, but not the actual magnitude of prediction errors in the dependent variable's units (e.g., RMSE is needed for this).\n",
        "4.  **Doesn't Assess Model Appropriateness:** A high $R^2$ doesn't guarantee the chosen model (e.g., linear) is the correct form for the data, nor does it ensure that underlying assumptions are met.\n",
        "5.  **Doesn't Penalize Complexity:** It doesn't account for model complexity, which can be misleading if simpler models perform nearly as well."
      ],
      "metadata": {
        "id": "Zfpm51fwCMX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19. How would you interpret a large standard error for a regression coefficient?**"
      ],
      "metadata": {
        "id": "IVO1hZr9yghZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A large standard error for a regression coefficient suggests high variability in the estimated coefficient, meaning it's not very stable across different samples of data. Here’s what that could indicate:\n",
        "\n",
        "* **Weak Relationship:** If the standard error is large, the coefficient itself might not be statistically significant, implying the predictor variable has little impact on the dependent variable.\n",
        "\n",
        "* **Multicollinearity:** It can signal strong correlations between independent variables, which can inflate standard errors and make it harder to isolate each variable’s unique contribution.\n",
        "\n",
        "* **Small Sample Size:** With fewer data points, estimates become less precise, leading to greater variability in the coefficient.\n",
        "\n",
        "* **High Variance in Data:** If the data is noisy or has substantial variability, the model struggles to provide precise estimates."
      ],
      "metadata": {
        "id": "Jk4hZnBksu54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**"
      ],
      "metadata": {
        "id": "z7VWLHjOymdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity refers to the non-constant variability of residuals across different levels of an independent variable. In residual plots, it is often detected by a funnel-shaped pattern, where residuals spread out unevenly instead of maintaining a consistent random distribution.\n",
        "\n",
        "Addressing heteroscedasticity is crucial for ensuring accurate statistical inference in regression models. If left uncorrected, it can lead to biased standard errors, inefficient coefficient estimates, and unreliable predictions, ultimately compromising the validity of data-driven decisions."
      ],
      "metadata": {
        "id": "O8Xq59AiuYxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**"
      ],
      "metadata": {
        "id": "ObE1zqY3yqwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high R² in a Multiple Linear Regression model indicates that the model explains a large portion of the variance in the dependent variable. However, if the Adjusted R² is much lower, it suggests that some predictors in the model are not actually contributing meaningful information. This happens because R² increases with every added variable, even if that variable does not improve the model’s predictive power. Adjusted R², on the other hand, penalizes unnecessary predictors to provide a more accurate assessment.\n",
        "\n",
        "A low Adjusted R² compared to R² can indicate overfitting, where the model is too complex and includes irrelevant features, making it less generalizable to new data. To address this issue, you can perform feature selection, remove weak predictors, or use cross-validation to refine the model and improve its real-world performance."
      ],
      "metadata": {
        "id": "H_7CUXr4xfLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22. Why is it important to scale variables in Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "BpMT8wv8zT3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling variables in Multiple Linear Regression is important because it ensures that all features contribute appropriately to the model and prevents numerical issues. Here's why it matters:\n",
        "\n",
        "* **Avoids Biased Coefficients:** When predictor variables have different scales, those with larger values may disproportionately influence the model, even if they aren't actually more important.\n",
        "\n",
        "* **Improves Model Convergence:** Some optimization algorithms used in regression, like gradient descent, work better when variables are on a similar scale, helping the model train efficiently.\n",
        "\n",
        "* **Enhances Interpretability:** Standardizing variables (e.g., using Z-score scaling) makes regression coefficients easier to compare since all predictors are on the same scale.\n",
        "\n",
        "* **Mitigates Multicollinearity Effects:** When variables are highly correlated, scaling can reduce numerical instability in calculations."
      ],
      "metadata": {
        "id": "RAWsP7ev0MtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23. What is polynomial regression?**"
      ],
      "metadata": {
        "id": "GyTWG0a7zegG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is an extension of linear regression where the relationship between the independent and dependent variables is modeled as an n-degree polynomial rather than a straight line. This allows the regression model to capture non-linear patterns in the data that a simple linear model might miss."
      ],
      "metadata": {
        "id": "K9abIbiM06Y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24. How does polynomial regression differ from linear regression?**"
      ],
      "metadata": {
        "id": "ZC4Ccbo3zrqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression differs from linear regression primarily in how it models relationships between variables.\n",
        "\n",
        "Linear regression assumes a straight-line relationship between the independent and dependent variables, following the equation\n",
        "$y = 𝑚𝑥 + c$. It is ideal for data that follows a linear trend.\n",
        "\n",
        "Polynomial regression extends linear regression by incorporating higher-degree terms, such as $y = ax^2 + bx + c$ or beyond. This allows it to model curved relationships, making it useful for capturing more complex patterns in data."
      ],
      "metadata": {
        "id": "MOA93VKS1VOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25. When is polynomial regression used?**"
      ],
      "metadata": {
        "id": "-H3e4887zx-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is used when the relationship between the independent and dependent variables is non-linear, meaning a straight-line model (linear regression) cannot accurately capture the trend."
      ],
      "metadata": {
        "id": "shQ4jZVq2nAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q26. What is the general equation for polynomial regression?**"
      ],
      "metadata": {
        "id": "SXXDje-7z8Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The general equation for polynomial regression extends the linear regression model by including higher-degree terms of the independent variable. It is expressed as:\n",
        "\n",
        "$y = a_nx^n + a_n-1x^n-1 + ⋯ + a_2 x^2 + a_1x + a_0$\n",
        "\n",
        "Where:\n",
        "\n",
        "* y is the dependent variable (target).\n",
        "\n",
        "* x is the independent variable (predictor).\n",
        "\n",
        "* a_n, a_n-1, ... a_1, a_0 are the regression coefficients.\n",
        "\n",
        "* n is the polynomial degree, determining the model's flexibility."
      ],
      "metadata": {
        "id": "Yvrbq-Ph26di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q27. Can polynomial regression be applied to multiple variables?**"
      ],
      "metadata": {
        "id": "eUtUMbW60Eq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, polynomial regression can be applied to multiple variables, extending the concept beyond a single predictor. This is known as multivariate polynomial regression, where the model incorporates polynomial terms for multiple independent variables."
      ],
      "metadata": {
        "id": "_aa30dqM5cki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q28. What are the limitations of polynomial regression?**"
      ],
      "metadata": {
        "id": "RQC0vQSH0LaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is useful for modeling non-linear relationships, but it comes with several limitations:\n",
        "\n",
        "* **Overfitting Risk:** Higher-degree polynomials can fit the training data too closely, reducing generalizability to new data.\n",
        "\n",
        "* **Excessive Complexity:** As the polynomial degree increases, the model becomes harder to interpret and computationally expensive.\n",
        "\n",
        "* **Sensitive to Noise:** Polynomial regression can amplify small fluctuations in data, leading to unreliable predictions.\n",
        "\n",
        "* **Multicollinearity Issues:** Adding higher-order terms increases correlation among predictors, making coefficient estimation unstable.\n",
        "\n",
        "* **Extrapolation Problems:** Polynomial models can behave unpredictably outside the range of the training data, making long-term predictions unreliable."
      ],
      "metadata": {
        "id": "W52VJoRZ5mlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**"
      ],
      "metadata": {
        "id": "OmUDL4rF0bgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting the degree of a polynomial in regression, it's important to evaluate the model's fit using several methods to balance accuracy and complexity.\n",
        "\n",
        "Here are key techniques:\n",
        "\n",
        "* **Residual Analysis:** Check for patterns in residual plots.\n",
        "\n",
        "* **R² & Adjusted R²:** Ensure the model isn't overly complex.\n",
        "\n",
        "* **Cross-Validation:** Compare test errors across different degrees.\n",
        "\n",
        "* **AIC & BIC:** Penalize unnecessary complexity.\n",
        "\n",
        "* **RMSE & MAE:** Measure prediction accuracy.\n",
        "\n",
        "* **Extrapolation Check:** Test behavior on unseen data."
      ],
      "metadata": {
        "id": "AyHYB_ot7m9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q30. Why is visualization important in polynomial regression?**"
      ],
      "metadata": {
        "id": "ouUUY9Db0gQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization plays a crucial role in polynomial regression by helping to interpret the model, assess fit, and detect potential issues. Here's why it's essential:\n",
        "\n",
        "* **Understand Non-Linear Trends:** Reveals how well the model captures curvature.\n",
        "\n",
        "* **Detect Overfitting:** Shows excessive fluctuations in high-degree models.\n",
        "\n",
        "* **Analyze Residuals:** Ensures errors are randomly distributed.\n",
        "\n",
        "* **Compare Model Degrees:** Helps select the best polynomial fit."
      ],
      "metadata": {
        "id": "1Mlm5SAM8PRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q31. How is polynomial regression implemented in Python?**"
      ],
      "metadata": {
        "id": "WGe01qsv0kv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression in Python is implemented using libraries like Scikit-learn and NumPy. The key steps involve:\n",
        "\n",
        "* **Generating Data** - Creating a dataset with a non-linear relationship.\n",
        "\n",
        "* **Transforming Features** - Using Polynomial Features to expand input variables into polynomial terms.\n",
        "\n",
        "* **Fitting a Model** - Applying Linear Regression to the transformed features.\n",
        "\n",
        "* **Making Predictions** - Using the trained model to predict values.\n",
        "\n",
        "* **Visualizing Results** - Plotting the polynomial curve to assess fit."
      ],
      "metadata": {
        "id": "8mAsB7xe8vRB"
      }
    }
  ]
}